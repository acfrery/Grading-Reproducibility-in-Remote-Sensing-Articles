% Encoding: UTF-8

@Article{ManifestoReproducibleScience,
  author    = {M. R. Munaf{\`{o}} and B. A. Nosek and D. V. M. Bishop and K. S. Button and C. D. Chambers and N. Percie du Sert and U. Simonsohn and E.-J. Wagenmakers and J. J. Ware and J. P. A. Ioannidis},
  journal   = {Nature Human Behaviour},
  title     = {A manifesto for reproducible science},
  year      = {2017},
  month     = {jan},
  number    = {1},
  pages     = {0021},
  volume    = {1},
  abstract  = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  doi       = {10.1038/s41562-016-0021},
  publisher = {Springer Nature},
}


@TechReport{StatisticalAnalysesReproducibleResearch,
	author      = {R. Gentleman and D. {Temple Lang}},
	title       = {Statistical Analyses and Reproducible Research},
	institution = {Bioconductor Project},
	year        = {2004},
	number      = {Paper 2},
	address     = {The Berkeley Electronic Press},
	keywords    = {Compendium, Dynamic documents, Literate programming, Markup language, Perl, Python, R.},
	url         = {http://www.bepress.com/bioconductor/paper2},
}

@Article{RRComputationalHarmonicAnalysis,
  author   = {D. L. Donoho and A. Maleki and I. U. Rahman and M. Shahram and V. Stodden},
  journal  = {Computing in Science \& Engineering},
  title    = {Reproducible Research in Computational Harmonic Analysis},
  year     = {2009},
  pages    = {8--18},
  volume   = {11},
  abstract = {Scientific computation is emerging as absolutely central to the scientific method. Unfortunately, it's error-prone and currently immature-traditional scientific publication is incapable of finding and rooting out errors in scientific computation-which must be recognized as a crisis. An important recent development and a necessary response to the crisis is reproducible computational research in which researchers publish the article along with the full computational environment that produces the results. In this article, the authors review their approach and how it has evolved over time, discussing the arguments for and against working reproducibly.},
  doi      = {10.1109/MCSE.2009.15},
}

@Article{RREconometrics,
  author   = {R. Koenker and A. Zeileis},
  journal  = {Journal of Applied Econometrics},
  title    = {On reproducible econometric research},
  year     = {2009},
  pages    = {833--847},
  volume   = {24},
  abstract = {Recent software developments are reviewed from the vantage point of reproducible econometric research.
We argue that the emergence of new tools, particularly in the open-source community, have greatly eased the burden of documenting and archiving both empirical and simulation work in econometrics. Some of these tools are highlighted in the discussion of two small replication exercises.},
  doi      = {10.1002/jae.1083},
}

@Article{RRSignalProcessing,
  author   = {Vandewalle, P. and Kovacevic, J. and Vetterli, M.;},
  journal  = {{IEEE} Signal Processing Magazine},
  title    = {Reproducible research in signal processing},
  year     = {2009},
  number   = {3},
  pages    = {37--47},
  volume   = {26},
  abstract = {What should we do to raise the quality of signal processing publications to an even higher level? We believe it to be crucial to maintain the precision in describing our work in publications, ensured through a high-quality reviewing process. We also believe that if the experiments are performed on a large data set, the algorithm is compared to the state-of-the-art methods, the code and/or data are well documented and available online, we will all benefit and make it easier to build upon each other's work. It is a clear win-win situation for our community: we will have access to more and more algorithms and can spend time inventing new things rather than recreating existing ones.},
  doi      = {10.1109/MSP.2009.932122},
}

@Article{AddressingNeedDataCodeSharingComputationalScience,
  author   = {{Yale Law School Roundtable on Data and Code Sharing}},
  journal  = {Computing in Science \& Engineering},
  title    = {Reproducible Research: Addressing the Need for Data and Code Sharing in Computational Science},
  year     = {2010},
  month    = {September/October},
  pages    = {8--12},
  abstract = {Roundtable participants identified ways of making computational research details readily available, which is a crucial step in addressing the current credibility crisis.},
  doi      = {10.1109/MCSE.2010.113},
}

@Article{ReproducibleResearchinComputationalScience,
  author   = {R. D. Peng},
  journal  = {Science},
  title    = {Reproducible Research in Computational Science},
  year     = {2011},
  pages    = {1226-1227},
  volume   = {334},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  doi      = {10.1126/science.1213847},
}


@Article{TenRulesReproducibleComputationalResearch,
	Title                    = {Ten Simple Rules for Reproducible Computational Research},
	Author                   = {Sandve, G. K. and Nekrutenko, A. and Taylor, J. and Hovig, E.},
	Journal                  = {PLoS Computational Biology},
	Pages                    = {e1003285},
	Volume                   = {9},
	Year                     = {2013},
	Month                    = {Oct},
	Number                   = {10},
	
	Doi                      = {10.1371/journal.pcbi.1003285},
	Editor                   = {Bourne, Philip E.Editor},
	Publisher                = {Public Library of Science},
	Url                      = {http://dx.doi.org/10.1371/journal.pcbi.1003285}
}

@Article{EditorialGRSL2015,
  author     = {A. C. Frery},
  journal    = {IEEE Geoscience and Remote Sensing Letters},
  title      = {How To Successfully Make a Scientific Contribution Through {IEEE} {G}eoscience and {R}emote {S}ensing {L}etters},
  year       = {2015},
  month      = {June},
  number     = {6},
  pages      = {1167--1169},
  volume     = {12},
  abstract   = {After more than a year of serving as Editor-in-Chief, I have collected impressions from authors, reviewers, and Associate Editors about certain patterns that lead to having manuscripts accepted. This Editorial aims at sharing these impressions.},
  bdsk-url-1 = {http://www.grss-ieee.org/publications/letters/submission-hints/},
  bdsk-url-2 = {https://doi.org/10.1109/LGRS.2015.2404211},
  doi        = {10.1109/LGRS.2015.2404211},
  url        = {http://www.grss-ieee.org/publications/letters/submission-hints/},
}

@Article{TellItlikeItIs,
  journal   = {Nature Human Behaviour},
  title     = {Tell it like it is},
  year      = {2020},
  month     = {Jan.},
  number    = {1},
  pages     = {1--1},
  volume    = {4},
  abstract  = {Every research paper tells a story, but the pressure to provide ‘clean’ narratives is harmful for the scientific endeavour.},
  doi       = {10.1038/s41562-020-0818-9},
  url = {http://dx.doi.org/10.1038/s41562-020-0818-9},
  publisher = {Springer Science and Business Media {LLC}},
}


@Unpublished{PublishingComputationalResearchaReviewofInfrastructuresforReproducibleandTransparentScholarlyCommunication,
	author      = {Markus Konkol and Daniel N{\"u}st and Laura Goulier},
	title       = {Publishing computational research -- A review of infrastructures for reproducible and transparent scholarly communication},
	month       = jan,
	year        = {2020},
	abstract    = {Funding agencies increasingly ask applicants to include data and software management plans into proposals. In addition, the author guidelines of scientific journals and conferences more often include a statement on data availability, and some reviewers reject unreproducible submissions. This trend towards open science increases the pressure on authors to provide access to the source code and data underlying the computational results in their scientific papers. Still, publishing reproducible articles is a demanding task and not achieved simply by providing access to code scripts and data files. Consequently, several projects develop solutions to support the publication of executable analyses alongside articles considering the needs of the aforementioned stakeholders. The key contribution of this paper is a review of applications addressing the issue of publishing executable computational research results. We compare the approaches across properties relevant for the involved stakeholders, e.g., provided features and deployment options, and also critically discuss trends and limitations. The review can support publishers to decide which system to integrate into their submission process, editors to recommend tools for researchers, and authors of scientific papers to adhere to reproducibility principles.},
	date        = {2020-01-02},
	eprint      = {2001.00484v1},
	eprintclass = {cs.DL},
	eprinttype  = {arXiv},
	url        = {http://arxiv.org/pdf/2001.00484v1},
	keywords    = {cs.DL, A.1; D.1; E.0; I.0},
}

@Article{SevenReasonsWhyaUsersGuidetoTransparencyandReproducibility,
  author    = {Dalson {Figueiredo Filho} and Rodrigo Lins and Amanda Domingos and Nicole Janz and Lucas Silva},
  journal   = {Brazilian Political Science Review},
  title     = {Seven Reasons Why: A User's Guide to Transparency and Reproducibility},
  year      = {2019},
  number    = {2},
  volume    = {13},
  abstract  = {Despite a widespread agreement on the importance of transparency in science, a growing body of evidence suggests that both the natural and the social sciences are facing a reproducibility crisis. In this paper, we present seven reasons why journals and authors should implement — transparent guidelines. We argue that sharing replication materials, which include full disclosure of the methods used to collect and analyze data, the public availability of raw and manipulated data, in addition to computational scripts, may generate the following positive outcomes: 01. production of trustworthy empirical results, by preventing intentional frauds and avoiding honest mistakes; 02. making the writing and publishing of papers more efficient; 03. enhancing the reviewers’ ability to provide better evaluations; 04. enabling the continuity of academic work; 05. developing scientific reputation; 06. helping to learn data analysis; and 07. increasing the impact of scholarly work. In addition, we review the most recent computational tools to work reproducibly. With this paper, we hope to foster transparency within the political science scholarly community.},
  doi       = {10.1590/1981-3821201900020001},
  publisher = {{FapUNIFESP} ({SciELO})},
}

@InProceedings{OutoftheBoxReproducibilityaSurveyofMachineLearningPlatforms,
  author    = {Richard Isdahl and Odd Erik Gundersen},
  booktitle = {2019 15th International Conference on {eScience} ({eScience})},
  title     = {Out-of-the-Box Reproducibility: A Survey of Machine Learning Platforms},
  year      = {2019},
  month     = {Sep.},
  publisher = {{IEEE}},
  abstract  = {Even machine learning experiments that are fully conducted on computers are not necessarily reproducible. An increasing number of open source and commercial, closed source machine learning platforms are being developed that help address this problem. However, there is no standard for assessing and comparing which features are required to fully support reproducibility. We propose a quantitative method that alleviates this problem. Based on the proposed method we assess and compare the current state of the art machine learning platforms for how well they support making empirical results reproducible. Our results show that BEAT and Floydhub have the best support for reproducibility with Codalab and Kaggle as close contenders. The most commonly used machine learning platforms provided by the big tech companies have poor support for reproducibility.},
  doi       = {10.1109/escience.2019.00017},
  url = {http://dx.doi.org/10.1109/eScience.2019.00017}
}

@InCollection{ReproducibilityofScientificResults2018,
	author       = {Fidler, F. and Wilcox, J.},
	booktitle    = {The Stanford Encyclopedia of Philosophy},
	publisher    = {Metaphysics Research Lab, Stanford University},
	title        = {Reproducibility of Scientific Results},
	year         = {2018},
	edition      = {Winter 2018},
	editor       = {Edward N. Zalta},
	url = {https://plato.stanford.edu/archives/win2018/entries/scientific-reproducibility/}
}

@Article{ReproducibleResearchandGIScienceanEvaluationUsingAGILEConferencePapers,
  author    = {Daniel N{\"u}st and Carlos Granell and Barbara Hofer and Markus Konkol and Frank O. Ostermann and Rusne Sileryte and Valentina Cerutti},
  journal   = {{PeerJ}},
  title     = {Reproducible research and {GIScience}: an evaluation using {AGILE} conference papers},
  year      = {2018},
  month     = {jul},
  pages     = {e5072},
  volume    = {6},
  abstract  = {The demand for reproducible research is on the rise in disciplines concerned with data analysis and computational methods. Therefore, we reviewed current recommendations for reproducible research and translated them into criteria for assessing the reproducibility of articles in the field of geographic information science (GIScience). Using this criteria, we assessed a sample of GIScience studies from the Association of Geographic Information Laboratories in Europe (AGILE) conference series, and we collected feedback about the assessment from the study authors. Results from the author feedback indicate that although authors support the concept of performing reproducible research, the incentives for doing this in practice are too small. Therefore, we propose concrete actions for individual researchers and the GIScience conference series to improve transparency and reproducibility. For example, to support researchers in producing reproducible work, the GIScience conference series could offer awards and paper badges, provide author guidelines for computational research, and publish articles in Open Access formats.},
  doi       = {10.7717/peerj.5072},
  publisher = {{PeerJ}},
}

@Article{TheStateofReproducibilityintheComputationalGeosciences,
  author    = {Markus Konkol and Christian Kray and Max Pfeier},
  journal   = {International Journal of Geographical Information Science},
  title     = {The state of reproducibility in the computational geosciences},
  year      = {2013},
  month     = {apr},
  number    = {2},
  pages     = {408--429},
  volume    = {33},
  abstract  = {Reproducibility is a cornerstone of science and thus for geographicresearch as well. However, studies in other disciplines such asbiology have shown that published work is rarely reproducible.To assess the state of reproducibility, specifically computationalreproducibility (i.e. rerunning the analysis of a paper using theoriginal code), in geographic research, we asked geoscientistsabout this topic using three methods: a survey (n = 146), interviews(n = 9), and a focus group (n = 5). We asked participantsabout their understanding of open reproducible research (ORR),how much it is practiced, and what obstacles hinder ORR. Wefound that participants had different understandings of ORR andthat there are several obstacles for authors and readers (e.g. effort,lack of openness). Then, in order to complement the subjectivefeedback from the participants, we tried to reproduce the resultsof papers that use spatial statistics to address problems in thegeosciences. We selected 41 open access papers from Copernicusand Journal of Statistical Software and executed the R code. Indoing so, we identified several technical issues and specific issueswith the reproduced figures depicting the results. Based on thesefindings, we propose guidelines for authors to overcome theissues around reproducibility in the computational geosciences.},
  doi       = {10.1080/13658816.2018.1508687},
  publisher = {ResearchGate},
}


@Book{ReproducibilityandReplicabilityinScience2019,
	title     = {Reproducibility and Replicability in Science},
	publisher = {National Academies Press},
	year      = {2019},
  	author    = {{National Academies of Sciences, Engineering, and Medicine}},
	month     = {sep},
	doi       = {10.17226/25303},
	url = {http://nap.edu/25303},
	isbn = {978-0-309-48616-3},
	address   = "Washington, DC"
}

@Misc{LetterNewton,
  author   = {Isaac Newton and Robert Hooke},
  title    = {{I}saac {N}ewton letter to {R}obert {H}ooke},
  year     = {1675},
  abstract = {Letter from Isaac Newton to Robert Hooke from early in their harried correspondence. Newton here accepts Hooke's invitation for a private correspondence and a sort of collaboration, noting that "what is done before many witnesses is seldom without some further concern than that for truth: but what passes between friends in private usually deserves the name of consultation rather than contest." Newton also asks Hooke for critiques of his papers assuring him that, "I am not so much in love with philosophical productions but oft I can make them yield to equity and friendship."},
  url      = {https://digitallibrary.hsp.org/index.php/Detail/objects/9792},
}



@Misc{TerminologiesforReproducibleResearch,
	author      = {Lorena A. Barba},
	title       = {Terminologies for Reproducible Research},
	year        = {2018},
	abstract    = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
	date        = {2018-02-09},
	eprint      = {1802.03311v1},
	eprintclass = {cs.DL},
	eprinttype  = {arXiv},
	keywords    = {cs.DL},
	url         = {http://arxiv.org/pdf/1802.03311v1},
}

@Article{PraxisofReproducibleComputationalScience,
  author    = {Lorena A. Barba},
  journal   = {Computing in Science {\&} Engineering},
  title     = {Praxis of Reproducible Computational Science},
  year      = {2019},
  month     = {Jan.},
  number    = {1},
  pages     = {73--78},
  volume    = {21},
  abstract  = {Among the top challenges of reproducible computational science are the following: 1) creation, curation, usage, and publication of research software; 2) acceptance, adoption, and standardization of open-science practices; and 3) misalignment with academic incentive structures and institutional processes for career progression. I will mainly address the first two here, proposing a praxis of reproducible computational science.},
  doi       = {10.1109/mcse.2018.2881905},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Misc{SuccessfulScientificPublishingfromtheProjecttotheAdvertising,
  author       = {Frery, Alejandro C.},
  title        = {Successful scientific publishing: from the project to the advertising},
  year         = {2020},
  abstract     = {We review good scientific practices that may lead to more success and less stress of the active researcher. We will emphasize the importance of reproducibility in every step of the research process: from the conception and writing of the proposal to the publication and support of the results.},
  doi          = {10.13140/RG.2.2.20805.19687},
  language     = {en},
  organization = {IEEE GRSS Joint Hyderabad and Bangalore Chapters webinar},
  url          = {https://ieeemeetings.webex.com/ieeemeetings/ldr.php?RCID=825b9854c01cd5f07190a96b50284234},
}


@Book{WritingScientificSoftware,
	Title                    = {Writing Scientific Software: a guide to good style},
	Author                   = {S. Oliveira and D. Stewart},
	Publisher                = {Cambridge},
	Year                     = {2006},
}

@misc{IPOL,
	title = {{Image Processing On Line}},
	key = {IPOL},
	url = {http://www.ipol.im/},
	issn = {2105-1232},
	doi = {10.5201/ipol},
}


@Book{WritingforComputerScience,
	Title                    = {Writing for Computer Science},
	Author                   = {J. Zobel},
	Publisher                = {Springer},
	Year                     = {2005},
	Edition                  = {2},
}

@Article{DespecklingPolSARImageswithaStructureTensorFilter2019,
  author   = {D. Santana-Cedr{\'e}s and L. Gomez and L. Alvarez and A. C. Frery},
  journal  = {IEEE Geoscience and Remote Sensing Letters},
  title    = {Despeckling {P}ol{SAR} Images with a Structure Tensor Filter},
  year     = {2020},
  month    = {Feb.},
  number   = {2},
  pages    = {357--361},
  volume   = {17},
  abstract = {In this letter, we propose a new despeckling filter for fully polarimetric synthetic aperture radar (PolSAR) images defined by 3×3 complex Wishart distributions. We first generalize the well-known structure tensor to deal with PolSAR data which allows to efficiently measure the dominant direction and contrast of edges. The generalization includes stochastic distances defined in the space of the Wishart matrices. Then, we embed the formulation into an anisotropic diffusion-like schema to build a filter able to reduce speckle and preserve edges. We evaluate its performance through an innovative experimental setup that also includes Monte Carlo analysis. We compare the results with a state-of-the-art polarimetric filter.},
  doi      = {10.1109/LGRS.2019.2919452},
  groups   = {Journal Article},
  url      = {http://ctim.ulpgc.es/demo111/},
}

@book{osterwalder2010business,
	title={Business model generation: a handbook for visionaries, game changers, and challengers},
	author={Osterwalder, Alexander and Pigneur, Yves},
	publisher={John Wiley \& Sons},
	year={2010}
}

@Article{DeepLearningandProcessUnderstandingforDataDrivenEarthSystemScience,
  author    = {Markus Reichstein and Gustau Camps-Valls and Bjorn Stevens and Martin Jung and Joachim Denzler and Nuno Carvalhais and Prabhat},
  journal   = {Nature},
  title     = {Deep learning and process understanding for data-driven {E}arth system science},
  year      = {2019},
  month     = {Feb.},
  number    = {7743},
  pages     = {195--204},
  volume    = {566},
  abstract  = {Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning.},
  doi       = {10.1038/s41586-019-0912-1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{TheLegalFrameworkforReproducibleScientificResearchLicensingandCopyright,
  author    = {Victoria Stodden},
  journal   = {Computing in Science {\&} Engineering},
  title     = {The Legal Framework for Reproducible Scientific Research: Licensing and Copyright},
  year      = {2009},
  month     = {Jan.},
  number    = {1},
  pages     = {35--40},
  volume    = {11},
  abstract  = {As computational researchers increasingly make their results available in a reproducible way, and often outside the traditional journal publishing mechanism, questions naturally arise with regard to copyright, subsequent use and citation, and ownership rights in general. The growing number of scientists who release their research publicly face a gap in the current licensing and copyright structure, particularly on the Internet. Scientific research produces more than the final paper: The code, data structures, experimental design and parameters, documentation, and figures are all important for scholarship communication and result replication. The author proposes the reproducible research standard for scientific researchers to use for all components of their scholarship that should encourage reproducible scientific investigation through attribution, facilitate greater collaboration, and promote engagement of the larger community in scientific learning and discovery.},
  doi       = {10.1109/mcse.2009.19},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{TowardsaMethodologyforDevelopingEvidenceInformedManagementKnowledgebyMeansofSystematicReview,
  author    = {David Tranfield and David Denyer and Palminder Smart},
  journal   = {British Journal of Management},
  title     = {Towards a Methodology for Developing Evidence-Informed Management Knowledge by Means of Systematic Review},
  year      = {2003},
  month     = {Sep},
  number    = {3},
  pages     = {207--222},
  volume    = {14},
  abstract  = {Undertaking a review of the literature is an important part of any research project. The researcher both maps and assesses the relevant intellectual territory in order to specify a research question which will further develop the knowledge base. However, traditional ‘narrative’ reviews frequently lack thoroughness, and in many cases are not undertaken as genuine pieces of investigatory science. Consequently they can lack a means for making sense of what the collection of studies is saying. These reviews can be biased by the researcher and often lack rigour. Furthermore, the use of reviews of the available evidence to provide insights and guidance for intervention into operational needs of practitioners and policymakers has largely been of secondary importance. For practitioners, making sense of a mass of often‐contradictory evidence has become progressively harder. The quality of evidence underpinning decision‐making and action has been questioned, for inadequate or incomplete evidence seriously impedes policy formulation and implementation. In exploring ways in which evidence‐informed management reviews might be achieved, the authors evaluate the process of systematic review used in the medical sciences. Over the last fifteen years, medical science has attempted to improve the review process by synthesizing research in a systematic, transparent, and reproducible manner with the twin aims of enhancing the knowledge base and informing policymaking and practice. This paper evaluates the extent to which the process of systematic review can be applied to the management field in order to produce a reliable knowledge stock and enhanced practice by developing context‐sensitive research. The paper highlights the challenges in developing an appropriate methodology.},
  doi       = {10.1111/1467-8551.00375},
  publisher = {Wiley},
}

@Article{ReproducibilityandReplicabilityinSARRemoteSensing,
  author    = {Timo Balz and Fabio Rocca},
  journal   = {{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  title     = {Reproducibility and Replicability in {SAR} Remote Sensing},
  year      = {2020},
  month     = {June},
  pages     = {3834--3843},
  volume    = {13},
  abstract  = {Modern science is built on systematic experimentationand observation. Today’s form of communicating scientific resultswith written articles is the foundation we build ourwork on.The reproducibilityand replicability of the experiments and observationsare central to this process of validation. However, reproducibilityand replicability are not always guaranteed, sometimes referredto as “crisis of reproducibility.” We believe that remote sensing,in general, suffers from this crisis. To analyze the extent of thecrisis, we conducted a survey on the state of reproducibility inremote sensing. Based on this survey, we map the problem ofreproducibility with a focus on synthetic aperture radar remotesensing, as this is our area of research. We also give advice on howto improve reproducibility in remote sensing.},
  doi       = {10.1109/jstars.2020.3005912},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Comment{jabref-meta: databaseType:bibtex;}
